---
title: "`micromatch` package:"
author: "Ines Garmendia"
date: "`r Sys.Date()`"
output: html_vignette
vignette: >
  %\VignetteIndexEntry{micromatch package}
  %\VignetteEngine{knitr::rmarkdown}
  %\usepackage[utf8]{inputenc}
---

About this document
===================

<p align="justify">This is the main vignette for `micromatch` package. This package provides a set of utilities to ease the task of statistically matching independent microdata files, with a focus to official statistics.</p>

<p align="justify">The main methods in `micromatch` are described in two books (see [1] and [2]), and are a result of two Eurostat projects in Data Integration and Statistical Matching (see [3] and [4]).</p>

<p align="justify">This document has two main parts. In the first chapter the reader will find an overview of the main concepts in statistical matching methodology. The second chapter deals with the use of `micromatch` in practice.</p>

<p align="justify">`micromatch` package also provides a vignette with a case based on real data from Eustat, the Basque Statistical Office. This document can be found in the package documentation.</p>

Fundamentals of Statistical Matching
====================================

> Statistical matching provides ways for producing combined analyses or integrated indicators for independent surveys referred to the same population of interest, from data containing distinct observations and stored in separate files, but sharing a common block of information

<p align="justify"><strong>Statistical matching</strong> (also known as data fusion, data merging or synthetic matching) is a set of techniques for providing joint information on variables or indicators collected through multiple sources, usually, surveys drawn from the same population of interest. The potential benefits of this approach lie in the possibility to enhance the complementary use and analytical potential of existing data sources (see [5] A. Leulescu & M. Agafitei, 2013).</p>

<p align="justify">Statistical matching has been widely used in market research, to link consumer behavior data and media consumption data.</p>

<p align="justify">In official statistics, it can be used to link different aspects that are usually studied separately for the same target population (e.g. inhabitants in a country at a particular time). A unique questionnaire covering all aspects such as population health, income, consumption, labour market, social capital... is seldom conceived: such a questionnaire would be too long, leading to a higher response burden, and to poor quality.</p>

<p align="justify">A separate survey is usually conducted to study each specific aspect of the population, the drawback being that responses will eventually lie in separate files. Statistical matching tries to overcome this limitation by making use of the shared information between the sources (typical sociodemographic variables such as age, sex and family size), in order to infer some type of "new" knowledge about aspects collected separately.</p>

The starting point (the input)
------------------------------

> The basic assumption is that the number of individuals or units in both samples (i.e. the overlap) is negligible. The fundamental difference with respect to other methods such as "record linkage" is that in the latter, we have identical units and we wish to find a correspondence between them in order to link the files. In statistical matching, we "know" the units are different, but we "wish" to find similar ones.

<p align="justify">Consider two independent surveys conducted on the same population of interest, each of which produces measures regarding a specific aspect (e.g. living styles or consumer behavior).</p> 

<p align="justify">We suppose that the surveys share a block of variables (sociodemographic variables such as the age, sex, or family size). When putting observations from distinct sources together, a particular missing data pattern emerges due to the non-observed values (i.e. answers we don't have in one survey because they belong to the other survey, and viceversa), see Fig 1.</p>

<div align="center"><img src="./fig1.png"><figcaption>Fig 1. The starting point: a block of common variables (Z) and two block of specific, non-jointly-observed, variables (X and Y)</figcaption></div>

<p align="justify">The aim is to obtain integrated analyses or results relating the not-jointly observed variables (blocks X and Y in the figure); to achieve this, we make use of the common information between the files (block Z) in some systematic way.</p>

The results (the output)
------------------------

<p align="justify">After matching we will typically obtain (one of these) 2 types of results:</p>

* a "synthetic file", i.e. a single file which contains full information on the variables and all units from both sources. The file is called synthetic because it contains variables that are not a result of direct observation. 

* particular estimates regarding variables living in separate files. The user might wish to estimate a contingency table or correlation coefficient, or any parameter of interest regarding variables in separate files. 

The former is named the **micro** approach. The latter is the **macro** approach.

In the micro approach, the synthetic dataset is used to make statistical analyses combining the variables of interest.

The matching process
--------------------

<p align="justify">Regardless of the matching method itself —i.e. the computational method by means of which a synthetic file or direct parameter estimations are produced—, the matching task involves a series of steps:</p>

1. <p align="justify">The choice of target variables (X and Y), i.e. the variables observed separately in distinct surveys.</p>

2. <p align="justify">Identification of the variables shared by the sources, and the study of their degree of coherence taking into account not only the wording of questions (which can be different, leading to non-agreeable measures), but also the marginal distributions observed separately in the data files. Variables that fail to show a minimum degree of coherence must be discarded. This step can be time-consuming, but it can also be the key for a successful matching.</p>

3. <p align="justify">Possibly, discarding further variables that are not predictive (i.e. are not related to) for the target variables, or are redundant with others.</p>

4. <p align="justify">The choice of a matching framework (parametric, non-parametric, mixed...) in a specific setting (micro or macro), and applying a matching/imputation/estimation algorithm. The algorithm will make use of the chosen subset of shared variables in steps 2 and 3 (namely, the _common matching variables_) to relate target variables fixed in step 1 (the _specific variables_).</p>

5. A thorough validation of results.

Using `micromatch`
=================

> The idea is that user should start defining specific objects
that will contain not only the original data, but also some attributes related 
to the matching task. 
Each step will be solved by means of functions/methods that 
take those specific objects as parameters.
The benefit is that the user will only
need to pass the information once.

<p align="justify">This chapter shows how functions in `micromatch` may be used to solve a specific matching task in practice.</p>

<p align="justify">`R` packages such as `StatMatch` or `mice` provide algorithms to solve the statistical matching problem. `micromatch` does not offer new algorithms for matching; rather, it provides a _context_ where the matching process is made easier, independently of the chosen methodology. In this way, alternative methodologies are integrated in a common context.</p>

<p align="justify">`micromatch` uses S4 classes to create a hierarchy of objects specific for matching. In the following, we will see how objects from this hierarchy may be created, and how they can be used to solve different steps.</p>

A simple example
----------------

<p align="justify">To illustrate the use of `micromatch`, we will be using data frames `samp.A` and `samp.B` included in `StatMatch` package. These examples provide some artificial data simulating typical variables present in the European Union Statistics on Income and Living Conditions Survey (EU-SILC). For more information on these data files please refer to `StatMatch` package documentation (type `?samp.A` and `?samp.B`).</p>

```{r loadStatMatch, warning=FALSE, message=FALSE}
library(StatMatch)
data(samp.A) #loads data into workspace
data(samp.B) # load data to workspace
str(samp.A)
str(samp.B)
levels(samp.B$labour5) <- c("employee", "self-employed", "unemployed", "retirement", "other") # assign descriptive levels 
```

The independent sources `samp.A` and `samp.B`, separately contain:

* a shared block of variables: 

    + `HH.P.id`: unit identifier
    + `area5` and `urb`: geographic variables
    + `hsize` and `hsize5`: family size (numeric and categorized)
    + `age` and `c.age`: age (numeric and categorized)
    + `sex`: gender
    + `marital`: marital status
    + `edu7`: education level

* one specific variable in each of the files:

    + in file `samp.A`: `n.income` and `c.neti`, net personal income (numeric and categorized, measured in thousands of euros)  
    + in file `samp.B`: `labour5`, the person's self-defined economic status.

* a weight variable,`ww`

**Important Note** 

<p align="justify">Note that the data frames have been prepared so that the names of corresponding variables are the same in both files. (For example, the weight variable is named `ww` in both `samp.A` and `samp.B`). This is a good idea in general: prior to matching, the user should study the potential shared variables carefully, naming the "same" variables equally in both files. Also, in the case of categorical variables, levels should be aggregated and harmonized to achieve comparable groups.</p>

<p align="justify">From this starting point, in the following we will illustrate how the matching task can be tackled with `micromatch`, step by step.</p>

#### Step 1-Specific variables: 

<p align="justify">The specific (target) variables in `samp.A` and `samp.B` are the income and the labour status, and it is advisable to store their name in the `R` session. For this example, we will use the categorical version of variable income, `c.neti`:</p>

```{r}
varesp_A <- "c.neti" # specific variable in file samp.A
varesp_B <- "labour5" # specific variable in file samp.B
```

#### Step 2-Common variables: 

<p align="justify">The shared variables are the remaining variables (excluding the identifier, `HH.P.id`, and the weight variable, `ww`). For this example we will use the categorical versions of the variables, and a single geographic variable, `urb`:</p>

```{r}
varshared <- c("urb", "c.age", "hsize5", "sex", "marital", "edu7") # define shared variables
```

<p align="justify">There is also a weight variable, `ww`, with the same name in both files:</p>

```{r}
weights <- "ww" # weight variable (same name in samp.A and samp.B)
```

Now that all variables have been checked and given a role, the purpose of matching can be made concrete: 

> We want to relate variables "c.neti" and "labour5" by applying some matching
algorithm that will use some subset of "varshared" variables to produce a synthetic, 
complete file. 
Specifically, we will fill "samp.A" -the receptor file-, by adding
variable "labour5" from "samp.B" -the donor file-.

**A basic distinction: `receptor` and `donor` files**

<p align="justify">In many matching problems, the solution consists of one the files receiving (one or more) variables from the other, independent file. In such situations, the file receiving "new" information is called the `receptor`, whereas the file lending it is the `donor`.</p>

<p align="justify">In general, the file with less observations will be used as receptor. Otherwise, donor observations would have to be used many times to "fill" the bigger file.</p>

<p align="justify">In `micromatch`, we will assign roles to the original data frames by using two constructor functions: `receptor` and `donor`. We may also want to fill _both_ files, meaning that the files will have symmetric roles (i.e. both receive and lend variables). For such cases we will use the `symmetric` constructor function, for both files.</p>

<p align="justify">Whatever the case is, the first task in `micromatch` will be to create a pair of `receptor` and `donor` objects (or two `symmetric` objects, not shown here):</p>

```{r constructObjets, message=FALSE}
library(micromatch)
# create the receptor object
rec <- receptor(data = samp.A, matchvars = varshared, specvars = varesp_A, weights=weights)
#
# create the donor object
don <- donor(data = samp.B, matchvars = varshared, specvars = varesp_B, weights=weights)
```

Parameter (slot) values can be checked by using `str` function:

```{r checkValues}
str(rec)
str(don)
```

<p align="justify">In the class hierarchy, it is useful to know that `receptor`, `donor` and `symmetric` are all `filetomatch` objects, the only difference being that they have disctint value of `role`.</p>

#### Step 3-1 (assess coherence) 

<p align="justify">First we must inspect the concordance between the marginal distributions of the shared variable, observed in both files. In `micromatch` three kind of tools are available: frequency tables, plots and empirical disimilarity measures (as computed by `comp.prop` function in `StatMatch`).</p>

<p align="justify">Because we have previously stored information about each type of variable in `receptor` and `donor` objects, all we need is to choose some options in `compare_matchvars` method:</p>

* `type`: the wanted result, values `table`, `plot` or `measures`; 
* `cell_values`: for type `table` or `plot`, values `abs` (absolute numbers) or `rel` (relative, i.e. percents) 
* `weights`: Should weights be considered? Values `TRUE` or `FALSE`;
* `strata`: Should a strata variable be introduced? Values `TRUE` or `FALSE`. To be used when we want to study distributions separately for specific groups in the population (male and female, etc)

Now we will ask for the three types of results: tables, plots and disimilarity measures for the previously defined objects, `rec` and `don`:
 
```{r}
# tables
compare_matchvars(x = rec, y = don, type = "table", cell_values = 'abs', weights = TRUE)
# plots
compare_matchvars(x = rec, y = don, type = "plot", cell_values = 'rel', weights = TRUE)
# disimilarity measures
compare_matchvars(x = rec, y = don, type = "measures", weights = TRUE)
```

<p align="justify">Overall, the results indicate that the shared variables are highly concordant between the data frames, in terms of the inspected marginal distributions.</p>

Note that 4 types of empirical measures are used:

* Dissimilarity index or total variation distance, `tvd`

* Overlap between two distributions, `overlap`

* Bhattacharyya coefficient, `Bhatt`

* Hellinger's distance, `Hell`

<p align="justify">For more information on these measures, please refer to `StatMatch` (`?comp.prop`) or Agresti's book ([6]).</p>

<p align="justify">In the example, Hellinger's distance (`Hell`) is below 0.05 in all cases (a usual rule of thumb in statistical matching, see reference [5]).</p>

#### Step 3-2 (assess predictive value)

<p align="justify">Now we should assess the predictive value of the common variables with respect to the specific ones, in order to discard unnecessary information (i.e. variables that are not predictive).</p>

<p align="justify">In `micromatch`, we can use `predictvalue` which relies on `StatMatch` function `pw.assoc`. This function admits a single object each time, and returns 4 statistical association measures for all the combinations of variables, based on Chi-Square and others. (Note that currently `predictvalue` only accepts categorical variables):</p>

* Cramer's `V`

* Goodman-Kruskal `lambda`

* Goodman-Kruskal `tau`

* Theil's uncertainty coefficient `U`

For more information on these measures please refer to `StatMatch` (`?pw.assoc`) or Agresti's book ([6]).

```{r predictValue}
predictvalue(x = rec) # predictive value of matchvars for target variable ("c.neti") in file samp.A
predictvalue(x = don) # predictive value of matchvars for target variable ("labour5") in file samp.B
```

A simple, temptative choice would be to keep varibles `c.age`, `sex` and `edu7`. Also, it can be a good idea to introduce `sex` as a group or strata variable. 

**Note**

<p align="justify">The variable selection shown here is limited because it only considers pairwise relations. That is, it could be the case that two common variables `Z1`, `Z2` are discarded because they are not predictive for, say, target variable `Y` in  file `A`. However, we could be missing the point that the _combination_ of the two, (`Z1`, `Z2`) _could in fact_ be predictive for `Y`. In this sense, some multivariate procedure such as regression modelling or random forests could be used, these are not implemented in `micromatch`</p>

<p align="justify">Specifically for matching, there exists a specific approach to this problem, namely, the selection of variables based on the reduction of uncertainty. The idea is to choose the variables that most help to reduce the degree of uncertainty about the non-observed relation between the specific (target) variables. For the interested user, an additional paragraph has been included at the end of this vignette: Selection based on uncertainty reduction (for categorical variables).</p>

### Final selection of common variables

Based on the previous analyses we will keep variables `c.age` and `edu7` as `matchvars`, and introduce `sex` as `stratavars` (see the note above). 

These changes can be easily introduced by using `update` method. Note that the new objects must be stored in the session:

```{r update}
# update variables for file A (receptor)
rec1 <- update(x = rec, matchvars = c("c.age", "edu7"), stratavars = "sex") 
#
# update variables for file B (donor)
don1 <- update(x = don, matchvars = c("c.age", "edu7"), stratavars = "sex") 
```

#### Step 4 (apply a matching algorithm)

<p align="justify">In this example distance _hot-deck_ imputation will be used to fill the non-observed values (variable `labour5` from `samp.B`) in file `samp.A`. In this algorithm, for each record in the receptor file `samp.A`, a similar record is searched in the donor file `samp.B`, and its value of `labour5` (i.e. an observed value) is given to the record in `samp.A`</p>

<p align="justify">In `micromatch` we can use the `match.hotdek` function, which in turn calls to `NND.hotdeck` function in `StatMatch`. This function finds the closest donor record in `donor` for each record in `receptor`, based on the chosen matching variables. Usually the _exact match_ is not possible to attain (i.e. for the receptor records it is not possible to find donor records having exactly the same values in the matching variables), and a distance function is introduced to find the most similar pairs.</p>

<p align="justify">In the example, `c.age` and `edu7` will be used to find similar donors, these being the variables stored as `matchvars`. We will also indicate that the strata variable in `stratavars`, `sex` should be used to define separate matching groups. That is, we want the search to be made within levels of `sex`, i.e., separately for male and female:</p>

```{r matchHotDeck, message=FALSE}
# hot-deck distance matching
result <- match.hotdeck(x = rec1, y = don1, strata = TRUE)
```

This function inherits other options available in the original function (`NND.hotdeck` in `StatMatch`). The most important are `dist.fun` and `constr.alg`:

* `dist.fun`: Choice of distance function. Available options are “Manhattan” (aka “City block”; default), “Euclidean”, “Mahalanobis”,“exact” or “exact matching”, “Gower”, “minimax” or one of the distance functions available in `proxy` package. For more information check `?NND.hotdeck`

* `constr.alg`: `TRUE` or `FALSE`. Indicates if the algorithm should be constrined, i.e. donor records should be used only once to fill the receptor records.

<p align="justify">The procedure has two main steps. First (receptor, donor) pairs are formed which are similar in terms of `matchvars`. Second, value observed in the donor record to is picked to fill the receptor record in each pair. In this way, the `receptor` file is 'completed'. The function returns an object of type `fusedfile`, in which `receptor` data are stored with additional (imputed) columns (in the example, an unique column, `labour5`).</p>

<p align="justify">The completed data can be re-used for further computations by extracting and storing the data frame in the session, as follows. (In the example our case we store the new data contains the additional column `labour5` and is stored with the name `A.imputed`):</p>

```{r storeImputedData}
# Extract the new, 'complete' data and store it under the name 'A.imputed'
samp.A.imp <- slot(result, "data")
#
# First 6 records. 
# The last column contains the imputed values for variable 'labour5'.
head(samp.A.imp)
```

* TODO. Details about the receptor and donor pairs should be obtained by means of a `details` function.
* TODO. create a extractor function data("filetomatch"): avoid the use of slot()

#### Step 5 (validate results)

<p align="justify">Now we should assess the validity of the resulting data frame in terms of its usefulness to perform good estimations for the relation between not-jointly observed variables (in our case, person's net income, `c.neti` and self-defined labour status, `labour5`).</p>

<p align="justify">The first, reasonable validation should be to check the similarity of imputed versus observed marginal distributions. For this purpose, we can use `tabulate2cat`, `plot2cat` and `similarity2cat` functions in `micromatch`, which essentially provide the same functionality as `compare_matchvars` (see Step 3-1 above).</p>

<p align="justify">In our example the distribution for variable `labour5` in the original file `samp.B` whould be compared to the imputed variable in `samp.A.imp` file. In `tabulate2cat`, `plot2cat` and `similarity2cat` functions, data frames have to be introduced directly as parameter values: in the example, `samp.B` and `samp.A.imp`.</p>

* TODO. create `validate1` method that will act on rec.fused, don pairs with options type=table, plot or measures.

<p align="justify">The variable to be compared is `labour5` in both files. The distributions are based on weighted data i.e. using the weights variable `ww`:</p>

```{r validateFirstOrder}
# Comparison of imputed vs observed distribution for variable 'labour5'
#
# store names in the session (for convenience)
var <- "labour5"
weights <- "ww"
#
# Compute raw tables
tabulate2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights, cell_values = "rel")
#
# Plots with percents
plot2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights, cell_values = "rel") # blue bar corresponds to imputed values
#
# Empirical measures
similarity2cat(data_A = samp.B, data_B = samp.A.imp, var_A = var, var_B = var, weights_A = weights, weights_B = weights) 
```

<p align="justify">The results are quite acceptable, but we should also compare distributions conditioned on other variables common to both files.</p>

<p align="justify">For example, a natural comparison would be to check distributions conditioned on `sex`, which was in fact used as strata variable. This can be done in with the same functions, by subseting over strata values, as follows:</p>

```{r}
levels(samp.B$sex) # codes for gender: 1-male, 2-female, check ?samp.A
#
# Gender equal to "1" = male
similarity2cat(data_A = subset(samp.B, sex == "1"), data_B = subset(samp.A.imp, sex == "1"), var_A = var, var_B = var, weights_A = weights, weights_B = weights)
#
# Gender equal to "2" = female
similarity2cat(data_A = subset(samp.B, sex == "2"), data_B = subset(samp.A.imp, sex == "2"), var_A = var, var_B = var, weights_A = weights, weights_B = weights)
```

Results seem to be 'good' by strata too.

* TODO. implement option `strata` true, false in `validate1` method.


Additional features
-------------------

#### Evaluation of uncertainty

> After producing a synthetic file, how can be sure that the true relations between non-jointly observed variables correctly reflect the "true" relations in the population?

<p align="justify">In statistical matching, the validation should imply a bit more effort. The primary reason is the lack of information about the non-jointly observed information (`X` and `Y` variables): after the synthetic file is produced, there's no direct way to check whether it correctly reflects the "true" relations between not-jointly observed variables in the population.</p>

<p align="justify">In abscence of additional information to check the fidelity of the estimations —possibly in the form of a third independent file `C`, that may contain observations for all variables, maybe from a previous wave of the same surveys, and not too distant in time—, a recommended approach is to perform an uncertainty analysis</p>.

<p align="justify">Specifically, in the case of categorical variables, _Frechet bounds_ can be used to produce a range of possible values for relative frequencies in the contingency table of `X` vs `Y`. The idea is that the distributions `P(X|Z)`, `P(Y|Z)` and `P(Z)`, which are estimated from the available data, are used to limit the range of possible values for `P(X, Y)`; for more details, see [1] and last chapter `StatMatch` package vignette; also type `?Frechet.bounds.cat`.</p> 

<p align="justify">Frechet bounds can be computed by using `frechet.uncertainty` method. We need to pass the `receptor`and `donor` objects, and additionaly the name of the target variables for which we want Frechet bounds to be computed. In the example the only variables are `c.neti` and `labour5`. The function uses the common `Z` variables `matchvars` to derive a range of values for each combination of `c.neti` and `labour5` levels. Note that 2 types of output can be chosen:</p>

* a table: `print.f` equals to `tables` (default)
* a data frame: `print.f` equals to `data.frame`

```{r}
frechet.uncertainty(x = rec, y = don, var_x = "c.neti", var_y = "labour5", base = don, print.f = "data.frame")
```

<p align="justify">Note that a warning message is produced when the marginal distribution of `matchvars` is not comparable between the two data frames. Note, however, that 6 `Z` variables were introduced as `matchvars`, which makes the possibility to meet coherence harder.</p>

Note that two types of bounds are estimated:

* `low.u` and `up.u`: lower and upper bounds for relative frequencies without conditioning on the `Z` variables
* `low.cx` and `up.cx`: lower and upper bounds for relative frequencies, when conditioning on the `Z` variables

<p align="justify">Also, a special unique estimated relative frequency is computed, `CIA`, which stands for the _conditional independent assumption_. Under this assumption, variables `Z` are supposed to be sufficient to estimate the unobserved relation between `X` and `Y`. In this extreme case, the estimated relative frequency of each cell is the product of the marginals: `p(X=i,Y=k) = p(X=j|Z=i)*p(Y=k|Z=i)*p(Z=i)`.</p>

#### Selection based on uncertainty reduction

<p align="justify">Finally we will show how `uncert2vars` function can be used to study how _overall uncertainty_ based on Frechet bounds changes when varying the set of matching variables. Note that, although introducing more variables will eventually reduce the uncertainty, ideally a small subset of variables should be retained.</p>

<p align="justify">Following our example, we now use `uncert2vars` to extract the overall uncertainty level associated with the 'best' 12 combinations of variables in `varshared` for the strata defied by `sex`, as follows. Results indicate that it could be a good idea to select distinct variables for each strata value. Also, the overall uncertainty is higher for female:</p>

```{r selectionUncertainty, warning=FALSE}
# sex == 1 (Male)
uncert2vars(var_x = "c.neti", var_y = "labour5", data_A = subset(samp.A, sex=="1"), data_B = subset(samp.B, sex=="1"), weights_A = "ww", weights_B = "ww", matchvars = c("urb", "c.age", "hsize5", "marital", "edu7"), base = subset(samp.A,sex=="1"), n=6)
# sex == 2 (Female)
uncert2vars(var_x = "c.neti", var_y = "labour5", data_A = subset(samp.A, sex=="2"), data_B = subset(samp.B, sex=="2"), weights_A = "ww", weights_B = "ww", matchvars = c("urb", "c.age", "hsize5", "marital", "edu7"), base = subset(samp.A,sex=="2"), n=6)

```

References
==========

[1] D'Orazio, M., Di Zio, M., & Scanu, M. (2006). *Statistical matching: Theory and practice*. John Wiley & Sons.

[2] Rässler, S. (2002). *Statistical matching*. Springer.

[3] *Data Integration* ESSnet project. (http://www.cros-portal.eu/content/data-integration-finished)

[4] *ISAD* ESSnet project (http://www.cros-portal.eu/content/isad-finished)

[5] Leulescu A. & Agafitei, M. *Statistical matching: a model based approach for data integration*, Eurostat methodologies and working papers, 2013. (http://epp.eurostat.ec.europa.eu/cache/ITY_OFFPUB/KS-RA-13-020/EN/KS-RA-13-020-EN.PDF)
